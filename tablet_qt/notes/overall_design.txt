===============================================================================
CLIENT ARCHITECTURE
===============================================================================

- We can't run solely over the web; that would break when no Internet is
  available.

- It's very hard to get a good hybrid that will run over the web and on a
  standalone client; using Titanium, for example, was not easy and never ended
  up with a satisfactory web client.

- At which point, you may as well have a high-performance cross-platform client
  system, for low-performance devices, hence Qt/C++ (and there's potential to
  run even these over the web; e.g. Qt applications are starting [2017] to
  support being served over VNC [*]).

    [*] http://doc.qt.io/qt-4.8/qt-embedded-vnc.html
        https://raspberrypi.stackexchange.com/questions/23221/running-a-qt5-application-not-full-screen-use-vnc
        https://stackoverflow.com/questions/24517954/running-a-qt-app-over-the-web
        https://stackoverflow.com/questions/3240633/web-based-vnc-client
        https://stackoverflow.com/questions/5745356/getting-the-qt-vnc-server-to-work

===============================================================================
SERVER DATA DESIGN
===============================================================================

We're trying to satisfy quite a few constraints...

- Ability to enter data on tablet with less than the server's preferred ID
  requirements (hence the "upload" and "finalize" ID policies).

- Ability to run in minimal-ID and full-ID modes, as the server prefers (hence
  configurable ID.

- General principle of approaching a normalized database where possible, but
  with some exceptions:
    - the {device, era} server fields;
    - the _current field and related linking (history within the same table as
      data);
    - some extra copying of ID descriptions to "patient" record at upload, so
      that ID is absolutely secured if someone changes the ID descriptions.
      ... changed; this is a protection against administrator error, which is
          a pretty thin reason for some considerable de-normalization

- ALTERNATIVE HISTORY MODELS:
    - table with "current" field and history information
        - what we're using
        - some need to filter for research exports (ignoring non-current stuff)
    - history in separate shadow tables (profusion of tables; need to move
      data);
    - single giant audit table with fields like "table", "field", "value",
      "date"
    - bitemporal model, "date of entry" + "date we believe entry to be correct"
        https://www.simple-talk.com/sql/t-sql-programming/a-primer-on-managing-data-bitemporally/
        ... though picking "current" data requires a scan for maximum date
            per row
    - let's stick with what we have

- Ability to run without a server, or to switch servers, and to wipe data from
  the tablet after upload -- hence the server's extra {device, era} fields.
  The server maintains an integer PK system.

    - ALTERNATIVE KEY MODELS:
        - tablet becomes aware of server PK and uses that somehow
            - server has to report back PKs recordwise at upload
            - hard to switch servers
            - if modified version is uploaded, have to rewrite tablet keys
        - server maintains an additional PK system
            - server calculates these at upload
            - requires uploading in a way that respects dependencies (i.e.
              to upload a record B where B.a_fk = A.pk, A has to be uploaded
              first).
            - server needs to understand the details of all records - not just
              Patient references, but e.g. if a Trial references its Task, the
              server would have to cope.
    - Let's stick.
    - That means we have to cope with COMPOSITE KEYS.

===============================================================================
SERVER ORM AND WEB FRAMEWORK
===============================================================================

- Then, that leads to a decision about server-side database ORM.
    - Let's move away from mine; we want better cross-database support.
    - SQLAlchemy one obvious candidate.
        - Can be customized for our ISO date/time field.
        - Migration support worse than Django (at least in terms of convenience).
    - Django the other.
        - Can be customized for our ISO date/time field.
        - Historical weaknesses re composite keys.
            - We don't need composite primary keys, but we do want composite
              foreign keys.
        - Django's notes re composite PKs:
            https://code.djangoproject.com/wiki/MultipleColumnPrimaryKeys
        - There is external support:
            https://pypi.python.org/pypi/django-composite-foreignkey
        - Maybe we can't do this:
            patient = models.ForeignKey(Patient, ... where current and same device/era...)
          but we could do this:

            def patient() -> Optional[Patient]:
                if self.anonymous():
                    return None
                q = Patient.objects.filter(id__eq=self.patient_id)
                q = q.filter(_current__eq=True)
                q = q.filter(_device__eq=self._device)
                q = q.filter(_era__eq=self._era)
                return q[0] if q else None
                # http://stackoverflow.com/questions/1387727/checking-for-empty-queryset-in-django

            def tasks_where_patient_female() -> QuerySet:
                for TaskClass in all_non_anonymous_task_classes():
                    # ... mmm... this is less obvious
                    # ...

- In terms of a general database framework, Django is pretty good, and it's
  not clear that Pyramid is a huge improvement. Django also brings:
    - the excellent admin interface
    - good migrations
    - its debug toolbar
    - good documentation
    - we know it operates in the Windows (e.g. SQL Server) environment
    - field comments ("help_text" attribute)
        https://docs.djangoproject.com/en/dev/ref/models/fields/#help-text
    - permitted values ("choices")
    - note it has a preference for using "" not NULL for empty strings,
      but it can cope:
        https://docs.djangoproject.com/en/dev/ref/models/fields/#null
    - you can always use raw SQL
        https://docs.djangoproject.com/en/1.11/topics/db/sql

- What's the best with SQLAlchemy?
    - Flask? http://flask.pocoo.org/
    - Pyramid? https://trypyramid.com/

        migrations: SQLAlchemy/Alembic: http://alembic.zzzcomputing.com/en/latest/tutorial.html
        debug toolbar: http://docs.pylonsproject.org/projects/pyramid_debugtoolbar/en/latest/

- An option is Django with composite keys hacked in:
    http://django-composite-foreignkey.readthedocs.io/
    ... that might do the trick!

- Automatic forms are a Django strength, including the "choices" option on a
  model.
  Equivalent in SQLAlchemy:
    http://sqlalchemy-utils.readthedocs.io/en/latest/data_types.html#module-sqlalchemy_utils.types.choice
    ... Pyramid... Deform... Colander... it's all a bit DIY.

- So in essence we have the advantages with:

    ORM             SQLAlchemy e.g. Pyramid
            ... unless multiple column foreign keys are here?
            ... https://www.quora.com/Which-is-better-and-why-Djangos-ORM-or-SQLAlchemy
            ... aha!
                https://github.com/django/django/blob/master/django/db/models/fields/related.py
                ForeignObject
                "Abstraction of the ForeignKey relation to support multi-column relations."
                But in tests/foreign_objects/tests.py:
                # ForeignObject is not part of public API.

    Templates       Equal (can mix and match with either)
    Forms           Django
    Migrations      Django
    Config files    Pyramid (via .ini), probably, but Django .py can read .ini...
    Serving static                              Pyramid?
    Extending from an ad-hoc code structure     Pyramid

  Ultimately, MIGRATIONS ARE IMPORTANT. Django does these very well, including
  asking for default values for old records when you create a new NOT NULL
  column and so forth. Though you can customize the SQL...

- Note also that our direct-to-database approach for the current API (rather
  than the user front end) talks directly to databases without any object-
  oriented Task stuff. That would be significantly harder in Django than
  SQLAlchemy, becaues whereas SQLAlchemy has its Core underneath its ORM,
  the next thing under Django's ORM is raw (potentially backend-specific) SQL.

- One might also consider writing the server in C++ and sharing the task-
  specific code, which from that standpoint might be a considerable advantage.
  For example, the relevant task calculations would be automatically
  consistent. You could inspect the fancier result views on the client.
  Introspection would be harder.
  One would use it directly or (realistically) load it and proxy Apache to it.
  Stuff like ZIP file autocreation might be mildly fiddly once but not a
  problem. Templating was never a big problem to start with. One could somehow
  hack the DatabaseObject class so that it extended its fields when in use
  on the server (though that would need different PK systems; not entirely
  trivial). The major questions that occur to me are:
    - URL routing
    - multithreading/routing requests to lots of server threads
    - HTTP response objects
    - cross-platform/cross-backend database access
      ... Qt: ODBC support, MySQL, etc.
    - forms and other HTTP stuff
    - graphs
      ... Qt: https://doc.qt.io/qt-6.5/qtcharts-barchart-example.html
    - PDF generation
      ... Qt: https://stackoverflow.com/questions/10697228/converting-html-to-pdf-with-qt
    - automatic migrations
    - debugging
    - we're not in the same league of ease of use compared to e.g. the Django
      or Pyramid debug toolbar!
  Relevant C++ frameworks include
    - CppCMS
    - Cutelyst
    - ULib
    - Treefrog
    - etc.:
      https://en.wikipedia.org/wiki/Comparison_of_web_frameworks#C.2B.2B_2
  Ultimately, this is moving into sledgehammer-for-nut territory.
  (Note also that the primary advantage, the DRY basis of things like complex
  algorithms, is a potential safety decrement; doing it twice in two languages
  allows an additional cross-check during development, for this particular,
  in principle more safety-critical thing.)

- Other comparisons:
  - https://www.airpair.com/python/posts/django-flask-pyramid
  - https://www.codementor.io/sheena/django-vs-pyramid-python-framework-comparison-du107yb1c

- DECISION:
        SQLAlchemy
            - with a custom Column-derived type to check "permitted values" in
              a non-obligatory way
            - migrations: Alembic
        Pyramid
            - forms: Deform

- Pyramid tutorials of note
  - https://docs.pylonsproject.org/projects/pyramid-cookbook/en/latest/database/sqlalchemy.html


===============================================================================
THINGS TO AVOID
===============================================================================

- Hacking around with summary tables; the server shouldn't be adding tables
  dynamically. Create the data dumps dynamically (and convert to SQL for
  downloading if required).

    - To get Django to report table creation SQL: "django-admin sqlmigrate"
      does this on the command line, but also:
      https://docs.djangoproject.com/en/1.11/ref/schema-editor/
      ... could run this on an in-memory SQLite connection...

        with connection.schema_editor() as schema_editor:
            schema_editor.create_model(MyModel)  # uses the database
            # ... or:
            for field in SOME_FIELD_LIST:
                column_sql, params = schema_editor.column_sql(MyModel, field)


===============================================================================
SERVER VERSION HISTORY AND JOINS
===============================================================================

CURRENT

- The server/tablet interaction causes the tablet to uploads stuff that's
  changed, on a per-table basis. So imagine the patient ID numbers, patient,
  task, and some sort of ancillary table, and imaging them like a desk, where
  uploading a new version places a new sheet of paper on top of the old:

                            version_3                           rec_3_v_3
    version_2               version_2               rec_2_v_2   rec_3_v_2
    version_1   version_1   version_1   rec_1_v_1   rec_2_v_1   rec_3_v_1
    ---------   ---------   ---------   ---------------------------------
      idnum      patient      task                  ancillary

  No, let's revise that metaphor a bit. It works for "the current state", which
  is "what's on top"...

  Everything from a device, at a particular time, will match on device ID and
  era. Now, finding the current set is easy:
    - (a) link as the tablet would do, on PKs/FKs;                     } "same
    - (b) add the additional criteria of matches on device ID and era; }  desk"
    - and (c) restrict to "current" records. } "topmost piece of paper"

CONTEMPORANEOUS 1: WHAT WAS THE STATE OF PLAY WHEN THE INDEX RECORD WAS
UPLOADED?

- The more subtle question is "show me the state of play at time X" where
  revisions may have occurred subsequently. Here, the desk metaphor breaks
  down. We might initially assume that the contemporaneous records are each
  "the topmost piece of paper no higher than our starting point", but that's
  wrong. (Once we have two stacks each of two pieces of paper, who knows what
  pairs occurred as they were dealt?) Imagine instead a desk in which every
  upload places a sheet of glass down before adding more paper. Let's just
  consider two tables:

        time_5      x4 [removed at: NULL]
        time_4      x3 [removed at: time_5]     y3 [removed at: NULL]
        time_3                                  y2 [removed at: time_4]
        time_2      x2 [removed at: time_4]
        time_1      x1 [removed at: time_2]     y1 [removed at: time_3]
                    ----                        -----
        added_at    this                        other

  So if we start with x2, then the contemporaneous y record is y1; x3 -> y3;
  y1 -> x1; and so on. As before, we begin with:

    - match on PK/FK links, as before;          } "same desk"
    - match on device ID and era, as before;    }

  Then the obvious part, for this specific question, is that a contemporaneous
  record can't have been added after we were:
    - AND other._when_added_batch_utc <= this._when_added_batch_utc

  and the slightly trickier part is that "other" can't have been removed before
  or at the time we were ADDED:
    - AND (other._when_removed_batch_utc > this._when_added_batch_utc
           OR _when_removed_batch_utc IS NULL)

  (when "this" was *removed* is irrelevant). So we end up with:

    -- Find what "other" looked like when "this" was uploaded
    SELECT other.*
    FROM other INNER JOIN this ON
        other.fk_to_this = this.pk
        AND other._device_id = this.device_id
        AND other._era = this._era

        AND other._when_added_batch_utc <= this._when_added_batch_utc
        AND (other._when_removed_batch_utc > this._when_added_batch_utc
             OR other._when_removed_batch_utc IS NULL)

  In other words, we take this._when_added_batch_utc at a fixed point in time
  and seek an "other" record whose timespan encompasses that point.

  Another metaphor: "I lived for 100 years. Who was alive on the day when I was
  born? Everyone who was born before or on my birthday, and who died after I
  was born or is still alive."

        b----d                      b-----d
             b-----d                      b---d
                   b------...                 b--------
                ^                             ^
                X---------...                 X--------

CONTEMPORANEOUS 2: WHAT WAS THE LAST STATE OF PLAY POSSIBLE FOR THIS INDEX
RECORD?

- A slightly different question: what was the latest revised version of the
  "other" object that could apply to "this"?
  In this example, x2 -> y1; x3 -> y3; y1 -> x2.
  This requires treating the lifespan of "this" as a time range.

  Our lifespan metaphor: "I may be alive or dead. Who was the last person in
  each [odd sequential non-overlapping family lineage] to see me alive?"
  If I'm dead:
    (*) Restrict to those born before I died.
    (*) Those who weren't already dead before I died.
  If I'm alive:
    (*) Restrict to those who are also still alive."

        b----d                      b---d               b-------d
             b----d                     b---------              b----...
                  b-----...
                  ^                              ^            ^
         ...------X                     B---------      ...---X
         I'm dead                       I'm alive       I'm dead


        b-----d     b-------d       b---d
              b-----d       b-------d   b-----
            B---------------------------------
        I'm alive

    So:

    -- Find what "other" looked like the last time "this" was valid
    SELECT other.*
    FROM other INNER JOIN this ON
        other.fk_to_this = this.pk
        AND other._device_id = this.device_id
        AND other._era = this._era

        AND (
            (
                this._when_removed_batch_utc IS NOT NULL  -- "I'm dead"
                AND other._when_added_batch_utc < this._when_removed_batch_utc
                AND (other._when_removed_batch_utc >= this._when_removed_batch_utc
                     OR other._when_removed_batch_utc IS NULL)
            )
            OR (
                this._when_removed_batch_utc IS NULL  -- "I'm alive"
                AND other._when_removed_batch_utc IS NULL
            )
        )

CONTEMPORANEOUS 3: IF I'D LOOKED AT THE TABLET AT A PARTICULAR TIME, WHAT WOULD
I HAVE SEEN?

- Rather than starting from an index record, now we start from an index time.
  We just take all relevant records (for a device/era) where

    _when_added_batch_utc <= index_time
    AND (_when_removed_batch_utc > index_time
         OR _when_removed_batch_utc IS NULL)

- This is a generalization of the "upload time of index record" version, above.

OVERALL QUESTION: WHAT'S USEFUL?

It is not clear that any of these things are really useful except as an audit
trail. They can certainly be confusing, and offer too many options (e.g. for
research dumps: current versions only? Everything?). All possible states of the
tablet that were uploaded can in principle be recreated.

Let's take the pragmatic approach: the server should offer views to
researchers (both the clinical-style per-patient views and the research-style
data dumps) of *current* data only.

This also allows us to use SQLAlchemy relationship() calls, which is a helpful
bonus.


===============================================================================
INITIAL NOTES ON GROUP CONCEPT, 2017
===============================================================================

- some sort of "group" concept, e.g. "affective disorder research database"
  plus "clinical" (or sometimes "all"), ?cf. Unix.

    - a group has one or more users
    - a user is in ONE OR MORE GROUPS

        e.g. 0 = Affective Disorders Research Database
             1 = Clinicians
             2 = Smith clinical lab

    - patient ID policy remains consistent across the server
    - tasks have a group ID (on the server, to keep the client simple)
      ... editable on the server, if required
      ... has to be per-task to cope with anonymous tasks
          ... or could be per-task for anonymous tasks and per-patient for
              everything else; that would greatly simplify moving tasks between
              groups (which should be a rare operation), because to be
              consistent afterwards for an identifiable task, all the patient
              information has to move with it (or be copied)
          ... but slows down task load, since group filter can't be applied
              in SQL
          ... and this would be simpler if "patient" records are not in groups,
              though that breaks segregation a bit
      ... could be in EVERY uploaded table
          ... perfect in terms of info segregation and fast SQL
                ... for which: _group IN (?, ?, ?, ...) [= user.permitted_groups()]
          ... works for bulk data dumps too (restrict by _group)
          ... makes moves complex, but they should be rare

    - a user chooses (on the web site) which group they upload to
      ... shouldn't be something that they need to change often

    - a group can see ITS OWN records +/- THOSE OF OTHER SPECIFIC GROUPS
        e.g. - members of the Clinicians group can see everything
             - members of the ADRD group can see their things only
        ... with a shortcut for "can see all"

    This allows various concepts:
    - a superset of "see your own +/- everyone's"
        = e.g. one group per user / one user per group, plus a "root" group
    - multiple experiments, with a group for each
        ... some users on individual experiments
        ... some on several
        ... lab head can see all
    - that in a clinical research context, where clinicians see everything
      from multiple research groups

    Fairly simple. The only question is how much the client should know.
    - If "nothing", which is the simplest:
        - users use web site if they want to switch groups
        - "does task exist" is tested by server within the context of the
          user's current group
        - ... which just means that users need to be aware that IF data has
          been *copied* to the server, it should probably be *moved* to the
          server prior to changing groups.
